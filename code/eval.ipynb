{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9669dfd-b412-4383-bca0-7606e490eefc",
   "metadata": {},
   "source": [
    "# Adaptive Code Generation\n",
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f6cd2-fa99-4e80-afa4-1231da66e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.methods import get_sampler, extract_code\n",
    "from llm.prompts.code_generation import PromptConstants, get_generic_question_template\n",
    "from evaluation.compute_code_generation_metrics import check_correctness, eval_sample, eval_problems, compute_eval_results\n",
    "from evaluation.code_generation import load_code_generation_dataset, Difficulty, CodeGenerationProblem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddddcf-7d15-41cb-82a2-bd26673205b3",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "* Choose between Apps and LiveCodeBench dataset\n",
    "  - Note: data types will be different as LCB generates a CodeGenerationProblem datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105d28a-b393-415a-a333-23ac2fac1aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load in Apps dataset of 300 sample test problems: in the order of introductory, interview, competitive\n",
    "dataset = load_dataset('json', data_files='./data/apps_test_300.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062e990-ea6f-426e-855f-b4eafe4101dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load in LiveCodeBench dataset given the release version\n",
    "\n",
    "dataset = load_code_generation_dataset(release_version=\"release_v5\", cutoff_date=\"2024-05-01\")\n",
    "easy_problems = [problem for problem in dataset if problem.difficulty == Difficulty.EASY]\n",
    "medium_problems = [problem for problem in dataset if problem.difficulty == Difficulty.MEDIUM]\n",
    "hard_problems = [problem for problem in dataset if problem.difficulty == Difficulty.HARD]\n",
    "\n",
    "print(f\"Number of easy problems: {len(easy_problems)}\")\n",
    "print(f\"Number of medium problems: {len(medium_problems)}\")\n",
    "print(f\"Number of hard problems: {len(hard_problems)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005d7c5-4876-4a0a-a05c-3ff1bbec670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to a slice for testing\n",
    "start_idx = 0\n",
    "end_idx = 10\n",
    "if isinstance(dataset[0], CodeGenerationProblem):\n",
    "    dataset = easy_problems[start_idx:end_idx] + medium_problems[start_idx:end_idx] + hard_problems[start_idx:end_idx]\n",
    "    print(f\"Using LiveCodeBench dataset of {len(dataset)} examples.\")\n",
    "else:\n",
    "    dataset = dataset.select(range(start_idx, end_idx, 1))\n",
    "    print(f\"Using Apps dataset of {dataset.num_rows} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59effc-3e0b-48db-8a61-fc2c6a524753",
   "metadata": {},
   "source": [
    "### Set system prompt and sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f731eb-9701-4d69-bb41-8e6579758913",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = PromptConstants.SYSTEM_MESSAGE_GENERIC\n",
    "method = get_sampler(\"sample_multiple\", \"gpt-4o\", temperature=1.3, n_samples=100, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0919c-7c1d-431d-98af-b94c54f095d6",
   "metadata": {},
   "source": [
    "### Code generation using sampler\n",
    "Generate code and extract it from response -- might need to do some error handling there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9adbeb-3385-42af-bc62-62cd5e64e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts and responses\n",
    "prompts = [get_generic_question_template(sample) for sample in dataset]\n",
    "responses = method(prompts)\n",
    "\n",
    "# Code generations as list of lists where each item within the list is a sampled generation for a prompt\n",
    "code_generations = [[extract_code(response) for response in responses[i]] for i in range(len(responses))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b35e5d-95b4-43db-8ea0-ea7c2dcf5e05",
   "metadata": {},
   "source": [
    "## Calculate diversity of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a5dee-7ecb-42dd-8fe8-9153f791feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_openai_embedding(code_samples):\n",
    "    sample_embeddings = []\n",
    "\n",
    "    # Embed all the code samples\n",
    "    for sample in code_samples:\n",
    "        \n",
    "        # Generate embeddings\n",
    "        response = client.embeddings.create(\n",
    "            input=sample,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        embedding = response.data[0].embedding\n",
    "        sample_embeddings.append(embedding)\n",
    "        \n",
    "    return sample_embeddings\n",
    "\n",
    "def generate_embeddings(code_generations):\n",
    "        code_embeddings = [None] * len(code_generations)\n",
    "        with ThreadPoolExecutor(max_workers=256) as executor:\n",
    "            futures = []\n",
    "            for i, code_samples in enumerate(code_generations):\n",
    "                future = executor.submit(generate_openai_embedding, code_samples)\n",
    "                futures.append((i, future))\n",
    "            \n",
    "            with tqdm(total=len(code_generations), desc=\"Processing requests\") as progress_bar:\n",
    "                for i, future in futures:\n",
    "                    code_embeddings[i] = future.result()\n",
    "                    progress_bar.update(1)\n",
    "        \n",
    "        return code_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de603589-0c8b-4368-8496-dbfdc86f15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_embeddings = generate_embeddings(code_generations)\n",
    "\n",
    "num_embeddings = len(code_embeddings)\n",
    "print(f\"Generated {num_embeddings} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2533a23-e684-49d8-98b8-c757da2348e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_nearest_neighbor_uniqueness(embeddings_list, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Computes uniqueness based on the nearest neighbor cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "    - embeddings (numpy.ndarray): Matrix of shape (N, D) where N is the number of items and D is embedding size.\n",
    "    - threshold (float): Similarity threshold below which an item is considered unique.\n",
    "    \n",
    "    Returns:\n",
    "    - uniqueness_score (float): Proportion of unique items in the dataset.\n",
    "    - nn_similarities (numpy.ndarray): Nearest neighbor similarities for each item.\n",
    "    \"\"\"\n",
    "    embeddings = np.array(embeddings_list)\n",
    "    \n",
    "    # Fit Nearest Neighbors model\n",
    "    nbrs = NearestNeighbors(n_neighbors=2, metric=\"cosine\").fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    \n",
    "    # Nearest neighbor similarity (1 - cosine distance)\n",
    "    nn_similarities = 1 - distances[:, 1]  # Exclude self (first neighbor is itself)\n",
    "    \n",
    "    # Count unique items (similarity below threshold)\n",
    "    unique_items = np.sum(nn_similarities < threshold)\n",
    "    \n",
    "    # Normalize uniqueness score\n",
    "    uniqueness_score = unique_items / len(embeddings)\n",
    "    \n",
    "    return uniqueness_score, nn_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3242348-7d52-4f7b-a54f-0cdd453fce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_unique_embeddings(embeddings_list, threshold=.9):\n",
    "    \"\"\"\n",
    "    Count unique embeddings based on cosine similarity threshold in a greedy way\n",
    "    \"\"\"\n",
    "    embeddings = np.array(embeddings_list)\n",
    "    n = len(embeddings)\n",
    "    \n",
    "    # Track which embeddings are already covered\n",
    "    is_unique = np.ones(n, dtype=bool)\n",
    "    unique_indices = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Skip if this embedding is already marked as a duplicate\n",
    "        if not is_unique[i]:\n",
    "            continue\n",
    "            \n",
    "        # Mark this as a unique embedding\n",
    "        unique_indices.append(i)\n",
    "        \n",
    "        # Calculate similarity to all other embeddings\n",
    "        similarities = cosine_similarity([embeddings[i]], embeddings)[0]\n",
    "        \n",
    "        # Mark similar embeddings as duplicates (excluding self)\n",
    "        for j in range(i+1, n):\n",
    "            if similarities[j] >= threshold and is_unique[j]:\n",
    "                is_unique[j] = False\n",
    "    \n",
    "    return len(unique_indices)\n",
    "\n",
    "def similarity_heatmap(embeddings_list, sample_size=100):\n",
    "    \"\"\"\n",
    "    Generate heatmap based on cosine similarities\n",
    "    \"\"\"\n",
    "    embeddings = np.array(embeddings_list)\n",
    "    \n",
    "    # Sample if too many embeddings\n",
    "    if len(embeddings) > sample_size:\n",
    "        indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "        sample = embeddings[indices]\n",
    "    else:\n",
    "        sample = embeddings\n",
    "        \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(sample)\n",
    "    \n",
    "    # Masking for better viewing\n",
    "    sim_matrix_masked = np.copy(sim_matrix)\n",
    "    mask = np.triu(np.ones_like(sim_matrix, dtype=bool), k=1)\n",
    "    sim_matrix_masked[mask] = np.nan # or can use 0 but that lowkey messes up scale\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(sim_matrix_masked, cmap='viridis')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.title('Embedding Similarity Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19a173-d3c1-4769-a1f1-ddf98a6a3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "unique_items = count_unique_embeddings(code_embeddings[index])\n",
    "#print(code_generations[index])\n",
    "print(unique_items)\n",
    "similarity_heatmap(code_embeddings[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55620d07-850c-4790-ae99-95b2696bfa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.90\n",
    "scores = []\n",
    "counts = []\n",
    "for i, embedding_samples in enumerate(code_embeddings):\n",
    "    score, similarities = compute_nearest_neighbor_uniqueness(embedding_samples, threshold=threshold)\n",
    "    unique_items = count_unique_embeddings(embedding_samples, threshold=threshold)\n",
    "    scores.append(score)\n",
    "    counts.append(unique_items)\n",
    "    print(f\"Problem {i}: Score:{score}, Unique Samples:{unique_items}\")\n",
    "\n",
    "sample_range = len(code_embeddings) / 3\n",
    "avg_easy_score = sum(scores[:10]) / sample_range\n",
    "avg_easy_count = sum(counts[:10]) / sample_range\n",
    "\n",
    "avg_med_score = sum(scores[10:20]) / sample_range\n",
    "avg_med_count = sum(counts[10:20]) / sample_range\n",
    "\n",
    "avg_hard_score = sum(scores[20:30]) / sample_range\n",
    "avg_hard_count = sum(counts[20:30]) / sample_range\n",
    "\n",
    "print(f\"Easy | Score:{avg_easy_score} | Count:{avg_easy_count}\")\n",
    "print(f\"Med | Score:{avg_med_score} | Count:{avg_med_count}\")\n",
    "print(f\"Hard | Score:{avg_hard_score} | Count:{avg_hard_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d19a7d-5a02-4c22-a3e6-e6a6ce525e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./experiments/diversity_clean.csv')\n",
    "\n",
    "mini_colors = {\n",
    "    0.1: '#6FA8DC', \n",
    "    0.7: '#3D85C6',  \n",
    "    1.3: '#0B5394'  \n",
    "}\n",
    "\n",
    "gpt4o_colors = {\n",
    "    0.1: '#F9A77B',  \n",
    "    0.7: '#F57D31', \n",
    "    1.3: '#C55A11'\n",
    "}\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100  \n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'Roboto', 'Lato', 'DejaVu Sans']\n",
    "\n",
    "# Create each plot based on the difficulty\n",
    "def create_plot(difficulty):\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    y_max = df[difficulty].max() * 1.1  \n",
    "    \n",
    "    # gpt-4o-mini\n",
    "    for temp in [0.1, 0.7, 1.3]:\n",
    "        mini_data = df[(df['model'] == 'gpt-4o-mini') & (df['temperature'] == temp)]\n",
    "        mini_data = mini_data.sort_values('count')  # Ensure data is sorted\n",
    "        ax.plot(mini_data['count'], mini_data[difficulty], \n",
    "                marker='o', linestyle='-', linewidth=3, markersize=10, \n",
    "                label=f'gpt-4o-mini (temp={temp})', \n",
    "                color=mini_colors[temp])\n",
    "    \n",
    "    # gpt-4o\n",
    "    for temp in [0.1, 0.7, 1.3]:\n",
    "        gpt4o_data = df[(df['model'] == 'gpt-4o') & (df['temperature'] == temp)]\n",
    "        gpt4o_data = gpt4o_data.sort_values('count')  # Ensure data is sorted\n",
    "        ax.plot(gpt4o_data['count'], gpt4o_data[difficulty], \n",
    "                marker='s', linestyle='-', linewidth=3, markersize=10, \n",
    "                label=f'gpt-4o (temp={temp})', \n",
    "                color=gpt4o_colors[temp])\n",
    "    \n",
    "    # plot settings\n",
    "    ax.set_xlabel('Sample Count', fontsize=16, fontweight='medium')\n",
    "    ax.set_ylabel('Unique Solutions Count', fontsize=16, fontweight='medium')\n",
    "    difficulty_title = difficulty.capitalize()\n",
    "    ax.set_title(f'Unique Solutions vs Sample Count ({difficulty_title} Problems)', \n",
    "              fontsize=18, fontweight='bold')\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(fontsize=12, loc='best', frameon=True)\n",
    "    ax.set_xticks([10, 50, 100])\n",
    "    ax.set_xticklabels([10, 50, 100], fontsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_ylim(0, y_max)\n",
    "    ax.text(0.02, 0.98, f\"Difficulty: {difficulty_title}\", \n",
    "            transform=ax.transAxes, fontsize=16, \n",
    "            fontweight='bold', va='top', ha='left',\n",
    "            bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Return the figure woo\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot the diversity\n",
    "fig_easy = create_plot('easy')\n",
    "plt.savefig('easy_solutions.png')\n",
    "plt.show()\n",
    "\n",
    "fig_medium = create_plot('medium')\n",
    "plt.savefig('medium_solutions.png')\n",
    "plt.show()\n",
    "\n",
    "fig_hard = create_plot('hard')\n",
    "plt.savefig('hard_solutions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbd292-4e2a-4814-890a-78965ae6286e",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluate code samples in a number of ways (really all used by one another, but listed for convenience anyways):\n",
    "1. check_correctness - single problem with single generation | params: sample, generation\n",
    "3. eval_sample - single problem with multiple generations | params: sample, generations (list of lists)\n",
    "4. eval_problems - across number of problems with various samples | params: samples, generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129eb54e-ea91-4b91-ac37-659833149b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_problems(dataset, code_generations, timeout=4)\n",
    "strict_accuracy, per_problem_accuracy = compute_eval_results(results, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67929ec7-0203-481c-b52f-dd4187870f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./experiments/experiments_apps/results_new/qwen7b_results.csv')\n",
    "df[\"Difficulty\"] = [\"DIFFICULTY.Introductory\"] * 620 + [\"DIFFICULTY.Interview\"] * 620 + [\"DIFFICULTY.Competition\"] * 620\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100  \n",
    "plt.rcParams['font.family'] = 'sans-serif'  # Use a clean, modern font\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'Roboto', 'Lato', 'DejaVu Sans']  # Modern fonts fallback list\n",
    "\n",
    "# Get unique difficulty levels and temperatures\n",
    "difficulty_levels = df[\"Difficulty\"].unique()\n",
    "temperature_levels = sorted(df[\"temperature\"].unique())\n",
    "print(temperature_levels)\n",
    "\n",
    "temp_color_map = {\n",
    "    temperature_levels[0]: \"#636EFA\",  # Blue\n",
    "    temperature_levels[1]: \"#2CA02C\",  # Green\n",
    "    temperature_levels[2]: \"#FF7F0E\",   # Orange\n",
    "    temperature_levels[3]: \"#8C564B\",  # Purple\n",
    "    temperature_levels[4]: \"#D62728\",   # Red\n",
    "    #temperature_levels[5]: \"#9803fc\",\n",
    "    #temperature_levels[6]: \"#fc03b1\",\n",
    "    #temperature_levels[7]: \"#400aa6\",\n",
    "    #temperature_levels[8]: \"#2a043d\"\n",
    "}\n",
    "\n",
    "#print(difficulty_levels)\n",
    "\n",
    "for difficulty in difficulty_levels:\n",
    "    plt.figure(figsize=(10, 7)) \n",
    "    subset = df[df[\"Difficulty\"] == difficulty]\n",
    "    \n",
    "    # Get unique sample counts within this difficulty\n",
    "    sample_counts = sorted(subset[\"num_samples\"].unique())\n",
    "    \n",
    "    for temp in temperature_levels:\n",
    "        if temp <= 1.3:\n",
    "            temp_subset = subset[subset[\"temperature\"] == temp]\n",
    "            temp_subset = temp_subset.groupby(\"num_samples\")[\"verified\"].mean().reset_index()\n",
    "            \n",
    "            # Sort by num_samples for consistent plotting\n",
    "            temp_subset = temp_subset.sort_values(\"num_samples\")\n",
    "            \n",
    "            plt.plot(temp_subset[\"num_samples\"], temp_subset[\"verified\"], \n",
    "                     marker='o', linestyle='-', linewidth=3, markersize=10, alpha=0.85, \n",
    "                     label=f\"Temp: {temp}\", color=temp_color_map[temp])\n",
    "    \n",
    "    # Set log scale for x-axis\n",
    "    #plt.xscale('log')\n",
    "    plt.xticks(sample_counts, sample_counts, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    #plt.xlabel(\"Number of Samples (Log Scale)\", fontsize=16, fontweight='medium')\n",
    "    plt.xlabel(\"Number of Samples\", fontsize=16, fontweight='medium')\n",
    "    plt.ylabel(\"Success Rate\", fontsize=16, fontweight='medium')\n",
    "    difficulty_label = str(difficulty)[11:]\n",
    "    plt.title(f\"Success Accuracy vs Sample Count ({difficulty_label})\", fontsize=18, fontweight='bold')\n",
    "\n",
    "    plt.legend(fontsize=13, loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6, which='both')  # Ensure grid aligns with log scale\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0df53-5da0-4f72-936a-1cba69ea0b1f",
   "metadata": {},
   "source": [
    "### Serial Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded25fb-d0c4-45ea-b367-e9aa844a1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Function to safely parse the solutions_generated column\n",
    "def parse_solutions(sol_str):\n",
    "    if sol_str == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(sol_str)[0]  # Assuming the first list in the nested list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Apply the parsing function\n",
    "df = pd.read_csv('./experiments/serial_refinement/results.csv')\n",
    "df['parsed_solutions'] = df['solutions_generated'].apply(parse_solutions)\n",
    "\n",
    "# Function to calculate success rates for groups of solutions\n",
    "def calculate_success_rates(row, n):\n",
    "    solutions = row['parsed_solutions']\n",
    "    if not solutions:\n",
    "        return 0\n",
    "    return int(any(solutions[:n]))  # Return 1 if any of the first n solutions are True\n",
    "\n",
    "# Create columns for different group sizes\n",
    "df['success_1'] = df.apply(lambda row: calculate_success_rates(row, 1), axis=1)\n",
    "df['success_5'] = df.apply(lambda row: calculate_success_rates(row, 5), axis=1)\n",
    "df['success_10'] = df.apply(lambda row: calculate_success_rates(row, 10), axis=1)\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'Roboto', 'Lato', 'DejaVu Sans']\n",
    "\n",
    "# Get unique difficulty levels and temperatures\n",
    "difficulty_levels = df[\"difficulty\"].unique()\n",
    "temperature_levels = sorted(df[\"temperature\"].unique())\n",
    "\n",
    "# Create color map\n",
    "temp_color_map = {\n",
    "    temperature_levels[0]: \"#636EFA\",  # Blue\n",
    "    temperature_levels[1]: \"#2CA02C\",  # Green\n",
    "    temperature_levels[2]: \"#FF7F0E\",  # Orange\n",
    "    temperature_levels[3]: \"#8C564B\",  # Brown\n",
    "    temperature_levels[4]: \"#D62728\",  # Red\n",
    "}\n",
    "\n",
    "# Use the provided data for 1 sample count\n",
    "easy_1 = [.72, .74, .74, .80, .78]\n",
    "medium_1 = [.17, .24, .37, .24, .20]\n",
    "hard_1 = [.03, .03, .05, .03, .05]\n",
    "\n",
    "# Map the provided data to difficulty levels\n",
    "provided_data = {\n",
    "    \"easy\": easy_1,\n",
    "    \"medium\": medium_1,\n",
    "    \"hard\": hard_1\n",
    "}\n",
    "\n",
    "# Group sizes for x-axis\n",
    "group_sizes = [1, 5, 10]\n",
    "\n",
    "# Create plots for each difficulty level\n",
    "for difficulty in difficulty_levels:\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    subset = df[df[\"difficulty\"] == difficulty]\n",
    "    \n",
    "    difficulty_label = difficulty.split('.')[-1] if '.' in difficulty else difficulty\n",
    "    difficulty_key = difficulty_label.lower()\n",
    "    \n",
    "    for i, temp in enumerate(temperature_levels):\n",
    "        temp_subset = subset[subset[\"temperature\"] == temp]\n",
    "        \n",
    "        # Calculate success rates for each group size\n",
    "        if difficulty_key in provided_data and i < len(provided_data[difficulty_key]):\n",
    "            # Use provided data for 1 sample\n",
    "            success_rates = [\n",
    "                provided_data[difficulty_key][i],\n",
    "                temp_subset['success_5'].mean(),\n",
    "                temp_subset['success_10'].mean()\n",
    "            ]\n",
    "        else:\n",
    "            success_rates = [\n",
    "                temp_subset['success_1'].mean(),\n",
    "                temp_subset['success_5'].mean(),\n",
    "                temp_subset['success_10'].mean()\n",
    "            ]\n",
    "        \n",
    "        plt.plot(group_sizes, success_rates,\n",
    "                 marker='o', linestyle='-', linewidth=3, markersize=10, alpha=0.85,\n",
    "                 label=f\"Temp: {temp}\", color=temp_color_map[temp])\n",
    "    \n",
    "    plt.xticks(group_sizes, group_sizes, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.xlabel(\"Number of Samples\", fontsize=16, fontweight='medium')\n",
    "    plt.ylabel(\"Success Rate\", fontsize=16, fontweight='medium')\n",
    "    plt.title(f\"Success Rate vs Sample Count ({difficulty_label})\", fontsize=18, fontweight='bold')\n",
    "    \n",
    "    plt.legend(fontsize=13, loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a0f2c-8110-4015-bca1-0f22d63eb22d",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
